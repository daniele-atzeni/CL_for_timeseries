{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "LWQsuk47G329",
        "outputId": "4db795eb-c528-48a3-e7e2-07d9c76ccb8b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "from gas import GASModel\n",
        "from utils import generate_dataset_multivariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xXHiMd3Kyib"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We try on the traffic_hourly dataset, a dataset with data coming from 862 sensors (aka features) placed in California's highways (more info at https://forecastingdata.org/), used also in \"Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The \"standard\" number of timesteps to predict is 48; no standard for the length of the pieces of the time series to feed to the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z2V8feJYKqJ6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/daniele/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"monash_tsf\", \"traffic_hourly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ookWRAoYK2TV"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "freq = \"1H\"\n",
        "prediction_length = 48\n",
        "\n",
        "from functools import lru_cache\n",
        "from functools import partial\n",
        "@lru_cache(10_000)\n",
        "def convert_to_pandas_period(date, freq):\n",
        "    return pd.Period(date, freq)\n",
        "def transform_start_field(batch, freq):\n",
        "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
        "    return batch\n",
        "\n",
        "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
        "test_dataset.set_transform(partial(transform_start_field, freq=freq))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jN-RwipFOyJb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/daniele/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
        "\n",
        "num_of_variates = len(train_dataset)\n",
        "\n",
        "train_grouper = MultivariateGrouper(max_target_dim=num_of_variates)\n",
        "test_grouper = MultivariateGrouper(\n",
        "    max_target_dim=num_of_variates,\n",
        "    num_test_dates=len(test_dataset) // num_of_variates, # number of rolling test windows\n",
        ")\n",
        "\n",
        "multi_variate_train_dataset = train_grouper(train_dataset)\n",
        "multi_variate_test_dataset = test_grouper(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((862, 17448), 1, (862, 17544), 1)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_variate_train_dataset[0]['target'].shape, len(multi_variate_train_dataset), multi_variate_test_dataset[0]['target'].shape, len(multi_variate_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N4rtNXgjV6ll"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([117, 862, 100]) torch.Size([117, 862, 48])\n"
          ]
        }
      ],
      "source": [
        "ts_length = 100\n",
        "x_train, y_train = generate_dataset_multivariate(multi_variate_train_dataset[0]['target'], ts_length, prediction_length)\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEDgfGxombWj"
      },
      "source": [
        "# Define Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R0eYms3-rhXp"
      },
      "outputs": [],
      "source": [
        "n_features = x_train.shape[1]\n",
        "\n",
        "# initialize gas params\n",
        "eta_mu = 0.999\n",
        "eta_sigma2 = 0.999\n",
        "\n",
        "# each input record is expected as (batch(=1!), n_features, ts_length)\n",
        "# the encoder of the time series is just a flattener of the time dimension\n",
        "ts_encoder = nn.Flatten()\n",
        "# the output model is a feedforward network\n",
        "# the output of ts_encoder in this case is n_feature * ts_length ##a flattener\n",
        "# the additional info is (batch, ts_length * 2 * n_features) ##A LOT!!\n",
        "# these vectors are concatenated, so the input_dim of the output net is:\n",
        "output_net_input_dim = n_features * ts_length + ts_length * 2 * n_features\n",
        "\n",
        "HID_SIZE_1 = 100\n",
        "HID_SIZE_2 = 100\n",
        "output_model = nn.Sequential(nn.Linear(output_net_input_dim, HID_SIZE_1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(HID_SIZE_1, HID_SIZE_2),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(HID_SIZE_2, n_features * prediction_length)\n",
        "                                )\n",
        "\n",
        "model = GASModel(ts_encoder, eta_mu, eta_sigma2, output_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TJRq-YzurhaG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 0.0103528826\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.MSELoss()\n",
        "LR = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "running_loss = 0.0\n",
        "\n",
        "for inputs, labels in zip(x_train, y_train):\n",
        "    # the first dimension must be batch_size (i.e. 1)\n",
        "    inputs = inputs.unsqueeze(0)\n",
        "    labels = labels.unsqueeze(0)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs.float())\n",
        "    # this output is (batch, n_features * prediction_length), we must reshape it\n",
        "    outputs = outputs.reshape((1, n_features, prediction_length))\n",
        "    loss = criterion(outputs, labels.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "print('[%d] loss: %.10f' % (1, running_loss / y_train.shape[0]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
