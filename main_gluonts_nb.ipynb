{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository import get_dataset\n",
    "\n",
    "from gluonts.mx import Trainer  # SimpleFeedForwardEstimator\n",
    "\n",
    "\n",
    "from my_simple_feedforward._estimator import SimpleFeedForwardEstimator\n",
    "from normalizer import GASSimpleGaussian\n",
    "from denormalizer import SumDenormalizer\n",
    "\n",
    "import torch\n",
    "\n",
    "dataset = get_dataset(\"m4_hourly\")\n",
    "CONTEXT_LENGTH = 100\n",
    "len_to_pred = (\n",
    "    dataset.metadata.prediction_length if dataset.metadata.prediction_length else 50\n",
    ")\n",
    "n_features = len(list(dataset.train))\n",
    "\n",
    "train_torch_list = [torch.from_numpy(el[\"target\"]) for el in dataset.train]\n",
    "max_len = max([ts.shape[0] for ts in train_torch_list])\n",
    "for i, el in enumerate(train_torch_list):\n",
    "    if el.shape[0] < max_len:\n",
    "        n_el_to_add = max_len - el.shape[0]\n",
    "        el_to_add = torch.zeros(n_el_to_add)\n",
    "        train_torch_list[i] = torch.cat([el, el_to_add])\n",
    "train_ts = torch.stack(train_torch_list, dim=1)\n",
    "\n",
    "test_torch_list = [torch.from_numpy(el[\"target\"]) for el in dataset.test]\n",
    "max_len = max([ts.shape[0] for ts in test_torch_list])\n",
    "for i, el in enumerate(test_torch_list):\n",
    "    if el.shape[0] < max_len:\n",
    "        n_el_to_add = max_len - el.shape[0]\n",
    "        el_to_add = torch.zeros(n_el_to_add)\n",
    "        test_torch_list[i] = torch.cat([el, el_to_add])\n",
    "test_ts = torch.stack(test_torch_list, dim=1)\n",
    "\n",
    "normalizer = GASSimpleGaussian()\n",
    "normalizer.warm_up(train_ts, test_ts)\n",
    "\n",
    "########\n",
    "# let's add the means as new features of the time series\n",
    "train_dataset = list(dataset.train)\n",
    "test_dataset = list(dataset.test)\n",
    "for i in range(normalizer.mus.shape[1]):\n",
    "    new_feature = train_dataset[0]\n",
    "    new_feature[\"target\"] = normalizer.mus[:, i]\n",
    "    train_dataset.append(new_feature)\n",
    "########\n",
    "\n",
    "input_dim = CONTEXT_LENGTH * n_features\n",
    "output_dim = len_to_pred * n_features\n",
    "denormalizer = SumDenormalizer(input_dim, output_dim)\n",
    "\n",
    "estimator = SimpleFeedForwardEstimator(\n",
    "    denormalizer,\n",
    "    num_hidden_dimensions=[10],\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    trainer=Trainer(ctx=\"cpu\", epochs=5, learning_rate=1e-3, num_batches_per_epoch=100),\n",
    ")\n",
    "\n",
    "predictor = estimator.train(dataset.train)\n",
    "\n",
    "\n",
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,  # test dataset\n",
    "    predictor=predictor,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
