{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gas import GASModel\n",
    "from utils import generate_timeseries, generate_dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid magic number; corrupt file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtraffic_hourly_train/data.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/torch/serialization.py:1035\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m magic_number \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1034\u001b[0m \u001b[39mif\u001b[39;00m magic_number \u001b[39m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid magic number; corrupt file?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1036\u001b[0m protocol_version \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m protocol_version \u001b[39m!=\u001b[39m PROTOCOL_VERSION:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid magic number; corrupt file?"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.load('traffic_hourly_train.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate time series and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROWING = True\n",
    "t, y = generate_timeseries(GROWING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the timeseries\n",
    "MAX_INDEX_TRAIN = 1500\n",
    "MAX_INDEX_TEST = 2500\n",
    "y_train = y[:MAX_INDEX_TRAIN]\n",
    "y_test = y[MAX_INDEX_TRAIN: MAX_INDEX_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the dataset as tensor of shape \n",
    "# (n_timeseries, ts_length, n_features) for inputs\n",
    "# (n_timeseries, el_to_predict) for labels\n",
    "TS_LENGTH = 200\n",
    "EL_TO_PREDICT = 50\n",
    "y_train, lab_train = generate_dataset(y_train, TS_LENGTH, EL_TO_PREDICT)\n",
    "y_test, lab_test = generate_dataset(y_test, TS_LENGTH, EL_TO_PREDICT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gas params\n",
    "eta_mu = 0.999\n",
    "eta_sigma2 = 0.999\n",
    "\n",
    "# the encoder of the time series is just a flattener of the time dimension\n",
    "ts_encoder = nn.Flatten()\n",
    "# the output model is a feedforward network\n",
    "# the output of ts_encoder is (batch, ts_length)\n",
    "# the additional info is (batch, ts_length * 2 * n_features)\n",
    "ts_embedding_dim = TS_LENGTH + 2*TS_LENGTH   \n",
    "\n",
    "HID_SIZE_1 = 100\n",
    "HID_SIZE_2 = 100\n",
    "output_model = nn.Sequential(nn.Linear(ts_embedding_dim, HID_SIZE_1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(HID_SIZE_1, HID_SIZE_2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(HID_SIZE_2, EL_TO_PREDICT)\n",
    "                                )\n",
    "\n",
    "model = GASModel(ts_encoder, eta_mu, eta_sigma2, output_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train, evaluate and plot result functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs, y_train, lab_train):\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in zip(y_train, lab_train):\n",
    "            # the first dimension must be batch_size (i.e. 1)\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('[%d] loss: %.10f' %\n",
    "                (epoch + 1, running_loss / y_train.shape[0]))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_model(model, y_all):\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for inputs in y_all:\n",
    "        inputs = inputs.unsqueeze(0)    # first dimension must be batch_size\n",
    "        outputs = model(inputs.float())\n",
    "        outputs = outputs.squeeze()   # no need for batch_size here\n",
    "        y_pred.append(outputs.detach().numpy())\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    #y_pred = y_pred.reshape(-1)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def plot_results(t, y, y_pred, max_index_test, max_index_train, ts_length):\n",
    "    #plot with plotly with a line where the training set ends\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=t[0:max_index_test], y=y, mode='lines', name='Actual'))\n",
    "    fig.add_trace(go.Scatter(x=t[ts_length:max_index_test], y=y_pred, mode='lines', name='Predicted'))\n",
    "    fig.add_trace(go.Scatter(x=[t[max_index_train], t[max_index_train]], y=[-20, 150], mode='lines', name='Training Set End'))\n",
    "    fig.update_layout(title='Dampened Sinusoid', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2240.5108971229\n",
      "[2] loss: 1683.2636131873\n",
      "[3] loss: 969.0713618352\n",
      "[4] loss: 453.9730130709\n",
      "[5] loss: 206.8542333750\n",
      "[6] loss: 96.2332986685\n",
      "[7] loss: 56.6985925528\n",
      "[8] loss: 44.8392334718\n",
      "[9] loss: 41.2561446887\n",
      "[10] loss: 39.6769336370\n",
      "[11] loss: 38.6195575641\n",
      "[12] loss: 37.6771610517\n",
      "[13] loss: 36.7620688769\n",
      "[14] loss: 35.8557845079\n",
      "[15] loss: 34.9555754845\n",
      "[16] loss: 34.0633565829\n",
      "[17] loss: 33.1894164636\n",
      "[18] loss: 32.3129578004\n",
      "[19] loss: 31.4503328984\n",
      "[20] loss: 30.5991178109\n",
      "[21] loss: 29.7587265235\n",
      "[22] loss: 28.9316369570\n",
      "[23] loss: 28.1228256134\n",
      "[24] loss: 27.3163325603\n",
      "[25] loss: 26.5268651797\n",
      "[26] loss: 25.7500879214\n",
      "[27] loss: 24.9983527018\n",
      "[28] loss: 24.2484442546\n",
      "[29] loss: 23.5165962348\n",
      "[30] loss: 22.7994652620\n",
      "[31] loss: 22.1010294969\n",
      "[32] loss: 21.4345033352\n",
      "[33] loss: 20.7506437852\n",
      "[34] loss: 20.0992734157\n",
      "[35] loss: 19.4669488027\n",
      "[36] loss: 18.8557694600\n",
      "[37] loss: 18.2535909323\n",
      "[38] loss: 17.6725446169\n",
      "[39] loss: 17.1087814478\n",
      "[40] loss: 16.5696251667\n",
      "[41] loss: 16.0429454125\n",
      "[42] loss: 15.5312198951\n",
      "[43] loss: 15.0310659775\n",
      "[44] loss: 14.5662894707\n",
      "[45] loss: 14.1040473168\n",
      "[46] loss: 13.6638876200\n",
      "[47] loss: 13.2422007964\n",
      "[48] loss: 12.8387112893\n",
      "[49] loss: 12.4467591231\n",
      "[50] loss: 11.9839708347\n",
      "[51] loss: 11.5711784179\n",
      "[52] loss: 11.1749072900\n",
      "[53] loss: 10.7071149074\n",
      "[54] loss: 10.2867594774\n",
      "[55] loss: 9.8848505937\n",
      "[56] loss: 9.5061097328\n",
      "[57] loss: 9.1529266009\n",
      "[58] loss: 8.8162513605\n",
      "[59] loss: 8.5027993092\n",
      "[60] loss: 8.1965019428\n",
      "[61] loss: 7.9011702079\n",
      "[62] loss: 7.6259733163\n",
      "[63] loss: 7.3189682135\n",
      "[64] loss: 7.0155845789\n",
      "[65] loss: 6.7557880787\n",
      "[66] loss: 6.4823776484\n",
      "[67] loss: 6.2465301018\n",
      "[68] loss: 6.0194846758\n",
      "[69] loss: 5.8212785629\n",
      "[70] loss: 5.6397023659\n",
      "[71] loss: 5.4747189467\n",
      "[72] loss: 5.3235117656\n",
      "[73] loss: 5.1874670799\n",
      "[74] loss: 5.0632588313\n",
      "[75] loss: 4.9513432429\n",
      "[76] loss: 4.8427722271\n",
      "[77] loss: 4.7448332218\n",
      "[78] loss: 4.6613751100\n",
      "[79] loss: 4.5884830952\n",
      "[80] loss: 4.5229435059\n",
      "[81] loss: 4.4654532212\n",
      "[82] loss: 4.4133785963\n",
      "[83] loss: 4.3716043050\n",
      "[84] loss: 4.3310626287\n",
      "[85] loss: 4.2946255666\n",
      "[86] loss: 4.2613103940\n",
      "[87] loss: 4.2363514900\n",
      "[88] loss: 4.2086298282\n",
      "[89] loss: 4.1852574899\n",
      "[90] loss: 4.1636810394\n",
      "[91] loss: 4.1454466673\n",
      "[92] loss: 4.1317550402\n",
      "[93] loss: 4.1164305394\n",
      "[94] loss: 4.1026433248\n",
      "[95] loss: 4.0919110225\n",
      "[96] loss: 4.0817449001\n",
      "[97] loss: 4.0720187792\n",
      "[98] loss: 4.0642134960\n",
      "[99] loss: 4.0572385696\n",
      "[100] loss: 4.0506756581\n",
      "[101] loss: 4.0465474404\n",
      "[102] loss: 4.0413833581\n",
      "[103] loss: 4.0365962615\n",
      "[104] loss: 4.0322440863\n",
      "[105] loss: 4.0290164856\n",
      "[106] loss: 4.0257472625\n",
      "[107] loss: 4.0219008189\n",
      "[108] loss: 4.0197353822\n",
      "[109] loss: 4.0169283243\n",
      "[110] loss: 4.0140814781\n",
      "[111] loss: 4.0129176470\n",
      "[112] loss: 4.0041819719\n",
      "[113] loss: 3.9976674869\n",
      "[114] loss: 3.9905311603\n",
      "[115] loss: 3.9874975131\n",
      "[116] loss: 3.9824122190\n",
      "[117] loss: 3.9798426353\n",
      "[118] loss: 3.9778072651\n",
      "[119] loss: 3.9768683635\n",
      "[120] loss: 3.9742324261\n",
      "[121] loss: 3.9724360613\n",
      "[122] loss: 3.9708146682\n",
      "[123] loss: 3.9690739742\n",
      "[124] loss: 3.9662467700\n",
      "[125] loss: 3.9643885447\n",
      "[126] loss: 3.9620472009\n",
      "[127] loss: 3.9605248800\n",
      "[128] loss: 3.9587331735\n",
      "[129] loss: 3.9559894525\n",
      "[130] loss: 3.9540915673\n",
      "[131] loss: 3.9521618990\n",
      "[132] loss: 3.9505989460\n",
      "[133] loss: 3.9467264781\n",
      "[134] loss: 3.9449663988\n",
      "[135] loss: 3.9430299906\n",
      "[136] loss: 3.9408019414\n",
      "[137] loss: 3.9386499203\n",
      "[138] loss: 3.9361054164\n",
      "[139] loss: 3.9349161295\n",
      "[140] loss: 3.9314617744\n",
      "[141] loss: 3.9295317118\n",
      "[142] loss: 3.9267891095\n",
      "[143] loss: 3.9244606862\n",
      "[144] loss: 3.9221160779\n",
      "[145] loss: 3.9199712643\n",
      "[146] loss: 3.9170587613\n",
      "[147] loss: 3.9144236950\n",
      "[148] loss: 3.9133049250\n",
      "[149] loss: 3.9096935987\n",
      "[150] loss: 3.9057284318\n",
      "[151] loss: 3.9042996535\n",
      "[152] loss: 3.9012230818\n",
      "[153] loss: 3.8990646142\n",
      "[154] loss: 3.8957436635\n",
      "[155] loss: 3.8943588642\n",
      "[156] loss: 3.8893731007\n",
      "[157] loss: 3.8860688668\n",
      "[158] loss: 3.8842476056\n",
      "[159] loss: 3.8815642137\n",
      "[160] loss: 3.8780284020\n",
      "[161] loss: 3.8745746246\n",
      "[162] loss: 3.8716467527\n",
      "[163] loss: 3.8682282429\n",
      "[164] loss: 3.8671743228\n",
      "[165] loss: 3.8620861035\n",
      "[166] loss: 3.8604881855\n",
      "[167] loss: 3.8547339531\n",
      "[168] loss: 3.8518039355\n",
      "[169] loss: 3.8492138661\n",
      "[170] loss: 3.8444556456\n",
      "[171] loss: 3.8426807202\n",
      "[172] loss: 3.8370440923\n",
      "[173] loss: 3.8334922515\n",
      "[174] loss: 3.8311950610\n",
      "[175] loss: 3.8273292046\n",
      "[176] loss: 3.8223752609\n",
      "[177] loss: 3.8193437595\n",
      "[178] loss: 3.8148478636\n",
      "[179] loss: 3.8129146924\n",
      "[180] loss: 3.8065429284\n",
      "[181] loss: 3.8043905588\n",
      "[182] loss: 3.7987188559\n",
      "[183] loss: 3.7955353810\n",
      "[184] loss: 3.7903641141\n",
      "[185] loss: 3.7886372254\n",
      "[186] loss: 3.7812799857\n",
      "[187] loss: 3.7776443775\n",
      "[188] loss: 3.7735973780\n",
      "[189] loss: 3.7697414481\n",
      "[190] loss: 3.7647008483\n",
      "[191] loss: 3.7601384337\n",
      "[192] loss: 3.7560173365\n",
      "[193] loss: 3.7527456971\n",
      "[194] loss: 3.7478482035\n",
      "[195] loss: 3.7417047712\n",
      "[196] loss: 3.7392979402\n",
      "[197] loss: 3.7328908810\n",
      "[198] loss: 3.7273252790\n",
      "[199] loss: 3.7253099221\n",
      "[200] loss: 3.7184485289\n",
      "[201] loss: 3.7129751260\n",
      "[202] loss: 3.7084301664\n",
      "[203] loss: 3.7034198000\n",
      "[204] loss: 3.6986614557\n",
      "[205] loss: 3.6932789546\n",
      "[206] loss: 3.6887838932\n",
      "[207] loss: 3.6850061371\n",
      "[208] loss: 3.6783835934\n",
      "[209] loss: 3.6722367452\n",
      "[210] loss: 3.6677783177\n",
      "[211] loss: 3.6622421512\n",
      "[212] loss: 3.6603877590\n",
      "[213] loss: 3.6537554356\n",
      "[214] loss: 3.6465442089\n",
      "[215] loss: 3.6414939073\n",
      "[216] loss: 3.6369891029\n",
      "[217] loss: 3.6301146287\n",
      "[218] loss: 3.6253338181\n",
      "[219] loss: 3.6197281021\n",
      "[220] loss: 3.6139865884\n",
      "[221] loss: 3.6081306659\n",
      "[222] loss: 3.6047306703\n",
      "[223] loss: 3.5989383872\n",
      "[224] loss: 3.5928670489\n",
      "[225] loss: 3.5863028673\n",
      "[226] loss: 3.5839512394\n",
      "[227] loss: 3.5745590329\n",
      "[228] loss: 3.5720337079\n",
      "[229] loss: 3.5636352988\n",
      "[230] loss: 3.5583557624\n",
      "[231] loss: 3.5515820613\n",
      "[232] loss: 3.5474961262\n",
      "[233] loss: 3.5404886328\n",
      "[234] loss: 3.5346994308\n",
      "[235] loss: 3.5286953633\n",
      "[236] loss: 3.5262379142\n",
      "[237] loss: 3.5168242179\n",
      "[238] loss: 3.5115059247\n",
      "[239] loss: 3.5042745379\n",
      "[240] loss: 3.5000769542\n",
      "[241] loss: 3.4958527272\n",
      "[242] loss: 3.4899785702\n",
      "[243] loss: 3.4815804362\n",
      "[244] loss: 3.4753659964\n",
      "[245] loss: 3.4696255464\n",
      "[246] loss: 3.4638431668\n",
      "[247] loss: 3.4567774213\n",
      "[248] loss: 3.4519898937\n",
      "[249] loss: 3.4475477750\n",
      "[250] loss: 3.4400973503\n",
      "[251] loss: 3.4321487087\n",
      "[252] loss: 3.4270251944\n",
      "[253] loss: 3.4209071031\n",
      "[254] loss: 3.4146357087\n",
      "[255] loss: 3.4113727533\n",
      "[256] loss: 3.4048329317\n",
      "[257] loss: 3.3959714495\n",
      "[258] loss: 3.3905391647\n",
      "[259] loss: 3.3836799447\n",
      "[260] loss: 3.3774103935\n",
      "[261] loss: 3.3715187495\n",
      "[262] loss: 3.3656240564\n",
      "[263] loss: 3.3605047969\n",
      "[264] loss: 3.3527720250\n",
      "[265] loss: 3.3468299371\n",
      "[266] loss: 3.3396750964\n",
      "[267] loss: 3.3341742571\n",
      "[268] loss: 3.3288182937\n",
      "[269] loss: 3.3208470757\n",
      "[270] loss: 3.3184449443\n",
      "[271] loss: 3.3081634595\n",
      "[272] loss: 3.3033910440\n",
      "[273] loss: 3.2961505881\n",
      "[274] loss: 3.2899738458\n",
      "[275] loss: 3.2846544064\n",
      "[276] loss: 3.2785982948\n",
      "[277] loss: 3.2698187507\n",
      "[278] loss: 3.2657582393\n",
      "[279] loss: 3.2584588344\n",
      "[280] loss: 3.2525581076\n",
      "[281] loss: 3.2481643191\n",
      "[282] loss: 3.2420785748\n",
      "[283] loss: 3.2355280289\n",
      "[284] loss: 3.2293604016\n",
      "[285] loss: 3.2227681417\n",
      "[286] loss: 3.2200211011\n",
      "[287] loss: 3.2131952185\n",
      "[288] loss: 3.2116198127\n",
      "[289] loss: 3.2020702545\n",
      "[290] loss: 3.2004018976\n",
      "[291] loss: 3.1947605885\n",
      "[292] loss: 3.1923767512\n",
      "[293] loss: 3.1900692353\n",
      "[294] loss: 3.1884955718\n",
      "[295] loss: 3.1879223860\n",
      "[296] loss: 3.1893196198\n",
      "[297] loss: 3.1893997284\n",
      "[298] loss: 3.1952825785\n",
      "[299] loss: 3.2005028862\n",
      "[300] loss: 3.2122382613\n",
      "[301] loss: 3.2246023050\n",
      "[302] loss: 3.2468543190\n",
      "[303] loss: 3.2772578001\n",
      "[304] loss: 3.3145067187\n",
      "[305] loss: 3.3670720229\n",
      "[306] loss: 3.4502009658\n",
      "[307] loss: 3.5394463172\n",
      "[308] loss: 3.6574394428\n",
      "[309] loss: 3.8152758800\n",
      "[310] loss: 4.0143405749\n",
      "[311] loss: 4.1123029773\n",
      "[312] loss: 3.9106609775\n",
      "[313] loss: 3.6109005763\n",
      "[314] loss: 3.4003512997\n",
      "[315] loss: 3.3195274197\n",
      "[316] loss: 3.2755061480\n",
      "[317] loss: 3.2590403649\n",
      "[318] loss: 3.2364379030\n",
      "[319] loss: 3.2280411353\n",
      "[320] loss: 3.2251130801\n",
      "[321] loss: 3.2075064778\n",
      "[322] loss: 3.2019969592\n",
      "[323] loss: 3.2029480842\n",
      "[324] loss: 3.1997425556\n",
      "[325] loss: 3.1946754547\n",
      "[326] loss: 3.1991375318\n",
      "[327] loss: 3.1888119945\n",
      "[328] loss: 3.1911770335\n",
      "[329] loss: 3.1988752805\n",
      "[330] loss: 3.2037517658\n",
      "[331] loss: 3.1990382488\n",
      "[332] loss: 3.2148210085\n",
      "[333] loss: 3.2203951753\n",
      "[334] loss: 3.2212600112\n",
      "[335] loss: 3.2380323914\n",
      "[336] loss: 3.2581467491\n",
      "[337] loss: 3.2608349461\n",
      "[338] loss: 3.2752743042\n",
      "[339] loss: 3.2951319218\n",
      "[340] loss: 3.3184595704\n",
      "[341] loss: 3.3455773317\n",
      "[342] loss: 3.3729411822\n",
      "[343] loss: 3.3995408187\n",
      "[344] loss: 3.4054106245\n",
      "[345] loss: 3.4187782269\n",
      "[346] loss: 3.4313156421\n",
      "[347] loss: 3.4385430171\n",
      "[348] loss: 3.4456977798\n",
      "[349] loss: 3.4556044019\n",
      "[350] loss: 3.4514916998\n",
      "[351] loss: 3.4536858751\n",
      "[352] loss: 3.4557143541\n",
      "[353] loss: 3.4458585473\n",
      "[354] loss: 3.4487073421\n",
      "[355] loss: 3.4390140909\n",
      "[356] loss: 3.4315766784\n",
      "[357] loss: 3.4196295371\n",
      "[358] loss: 3.4046508028\n",
      "[359] loss: 3.3917249900\n",
      "[360] loss: 3.3784717092\n",
      "[361] loss: 3.3364474452\n",
      "[362] loss: 3.3114590599\n",
      "[363] loss: 3.2941236267\n",
      "[364] loss: 3.2752488897\n",
      "[365] loss: 3.2479228882\n",
      "[366] loss: 3.2314758347\n",
      "[367] loss: 3.2203585230\n",
      "[368] loss: 3.2012068767\n",
      "[369] loss: 3.1846184501\n",
      "[370] loss: 3.1643409912\n",
      "[371] loss: 3.1434726348\n",
      "[372] loss: 3.1238802947\n",
      "[373] loss: 3.1020406897\n",
      "[374] loss: 3.0842801653\n",
      "[375] loss: 3.0665524464\n",
      "[376] loss: 3.0483670464\n",
      "[377] loss: 3.0323340664\n",
      "[378] loss: 3.0199376666\n",
      "[379] loss: 3.0053674945\n",
      "[380] loss: 2.9932145568\n",
      "[381] loss: 2.9858635893\n",
      "[382] loss: 2.9763048016\n",
      "[383] loss: 2.9698783389\n",
      "[384] loss: 2.9646234191\n",
      "[385] loss: 2.9571792575\n",
      "[386] loss: 2.9508813895\n",
      "[387] loss: 2.9449562705\n",
      "[388] loss: 2.9404514570\n",
      "[389] loss: 2.9336977280\n",
      "[390] loss: 2.9306610043\n",
      "[391] loss: 2.9254718973\n",
      "[392] loss: 2.9218865954\n",
      "[393] loss: 2.9184146844\n",
      "[394] loss: 2.9146082126\n",
      "[395] loss: 2.9123079868\n",
      "[396] loss: 2.9104052736\n",
      "[397] loss: 2.9081047315\n",
      "[398] loss: 2.9073833456\n",
      "[399] loss: 2.9050193200\n",
      "[400] loss: 2.9044614893\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "criterion = nn.MSELoss()\n",
    "LR = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "epochs = 400\n",
    "\n",
    "model = train_model(model, criterion, optimizer, epochs, y_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([46, 200, 1]), torch.Size([46, 50]), (46, 50))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on the entire dataset\n",
    "y_all, lab_all = generate_dataset(y, TS_LENGTH, EL_TO_PREDICT)\n",
    "\n",
    "y_pred = eval_model(model, y_all)\n",
    "y_all.shape, lab_all.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plot_results(t, y, y_pred, MAX_INDEX_TEST, MAX_INDEX_TRAIN, TS_LENGTH)\n",
      "\u001b[1;32m/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m fig\u001b[39m.\u001b[39madd_trace(go\u001b[39m.\u001b[39mScatter(x\u001b[39m=\u001b[39m[t[max_index_train], t[max_index_train]], y\u001b[39m=\u001b[39m[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m150\u001b[39m], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlines\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Set End\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDampened Sinusoid\u001b[39m\u001b[39m'\u001b[39m, xaxis_title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime (s)\u001b[39m\u001b[39m'\u001b[39m, yaxis_title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAmplitude\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/daniele/Desktop/CL_for_timeseries/test_gas.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/plotly/basedatatypes.py:3409\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3377\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3409\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/CL_for_timeseries/cl_venv/lib/python3.10/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "y_pred = y_pred.reshape(-1)\n",
    "plot_results(t, y, y_pred, MAX_INDEX_TEST, MAX_INDEX_TRAIN, TS_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434.03922102573944\n"
     ]
    }
   ],
   "source": [
    "#Compute the total error on the test set.\n",
    "error = 0\n",
    "for i in range(MAX_INDEX_TRAIN, MAX_INDEX_TEST):\n",
    "    error = error + (y[i] - y_pred[i-TS_LENGTH])**2\n",
    "error = error/(MAX_INDEX_TEST - MAX_INDEX_TRAIN)\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
